# 网络编程基础知识

#### 一次IO的两个阶段：

- ##### 数据准备（内核等待I/O设备准备数据）：

  - 阻塞：IO无数据，IO接口不返回，则阻塞当前线程等待。
  - 非阻塞：IO无数据，IO接口立即返回error，用户线程**轮询**访问IO接口查看是否有数据到来。

- ##### 数据读写（将数据从内核缓冲区拷贝到用户空间）：

  - 同步：用户**调用API**自行完成数据从内核到用户空间的拷贝。拷贝的过程中用户线程是阻塞的。
  - 异步：内核线程完成数据的拷贝，并在拷贝完成后对用户进行**信号**通知。用户需要**调用API**预先告知OS关注的事件以及通知方式。用户线程不必关心拷贝过程。



#### Unix五种IO模型

- 阻塞IO（同步阻塞）：使用系统调用，期间用户线程会一直阻塞到数据从内核复制到用户空间。

  ![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/8/16f842eb840ddb08~tplv-t2oaga2asx-zoom-in-crop-mark:4536:0:0:0.image)

- 非阻塞IO（同步非阻塞）：在数据准备阶段，需要通过系统调用不停轮询访问知道数据准备情况。

  ![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/8/16f842ee3a487466~tplv-t2oaga2asx-zoom-in-crop-mark:4536:0:0:0.image)

- IO复用（同步）：将轮询的工作交给内核来做，避免了用户态与内核态频繁切换带来的CPU消耗。数据准备好后通知用户线程进行系统调用获取数据。

  ![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/8/16f842f130c589b8~tplv-t2oaga2asx-zoom-in-crop-mark:4536:0:0:0.image)

- 信号驱动（同步）：数据准备阶段是通过事先注册的信号处理函数，使得内核在数据准备就绪时通过信号来进行通知。

  ![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/8/16f842f3ca64ecd9~tplv-t2oaga2asx-zoom-in-crop-mark:4536:0:0:0.image)

- 异步IO（非阻塞异步）：甩手掌柜。操作系统工作繁重，目前只有IOCP支持。

  ![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/8/16f842f7f818d1f3~tplv-t2oaga2asx-zoom-in-crop-mark:4536:0:0:0.image)

>暴论1：异步阻塞的IO模型不太可能存在——如果你阻塞等待数据到来只是为了告知操作系统帮你装填好数据后通知你，为什么不在一开始就要求操作系统完成一次IO后再通知呢？
>
>暴论2：IO复用好在哪里？IO复用是非阻塞IO的改良。相较于阻塞IO，它对数据准备和读写两个阶段做了解耦。这使得在面对多连接的情况下：阻塞IO需要处理完第一个连接的读写事件才能响应新的连接；而IO复用能将数据读写交给线程来做——当然你也可以阻塞IO+多线程，但是线程的个数是有上限的，而且多线程的维护成本与线程之间切换的开销不可忽视。这就支持了IO复用的方法：select/poll/epoll去实现监听多个套接字的能力。

有两点需要注意：

- 在面对单个连接的情况下，IO复用的效率是不如阻塞IO的（两次系统调用vs一次系统调用）。
- 虽然select/poll/epoll等IO复用方法一般会被设置成non-blocking，但是用户线程仍然会被select/poll/epoll这些方法阻塞。（不是socket IO阻塞的）



#### Select/Epoll的对比——论Epoll的优越性

当用户创建套接字socket的时候，os会相应地创建一个socket对象，这个对象包含有：发送缓冲区、接收缓冲区和**等待队列**等成员。

当线程调用recv等方法时，os就将线程的引用挂到对应socket的等待队列中，此时线程就会进入阻塞态。当socket上收到数据后，该线程引用就会从对应socket的等待队列中剔除，插入就绪队列中。

- select工作流程：

假设进程A同时监听sock1、sock2、sock3，select会**将进程A（的引用）同时加入这三个socket的等待队列中**。直到某一个socket上收到数据，中断程序就会将进程唤起——它会**被移除出所有socket的等待队列**。但此时select并**不知道是哪个socket上发生了事件**，它需要**对自己监听的sock列表进行遍历**。（这里的队列是数据结构意义上的普通链表）。

也就是这样一次IO，需要整整三次遍历操作。并且加入队列和移除出队列，都需要将整个sock列表传给内核。当队列越长，遍历耗时就越久，同时内核/用户空间的拷贝开销也就越大。因此出于效率的考量，默认情况下select最多只能监听1024个socket。

- epoll工作流程：

epoll在进程和socket之间加了一层eventpoll，当进程调用epoll_create时，内核会创造一个eventpoll对象，这个对象和socket对象一样，也是文件描述符，也拥有一个等待队列。

同时epoll将**“监视操作”**和**“等待操作”**进行分离。

epoll_ctl：当进程对sock1、sock2、sock3进行监视，内核会将进程的eventpoll对象添加到三个socket的等待队列中。

epoll_wait：进程阻塞，os会将进程A（的引用）放入eventpoll的等待队列中。当任一socket接收到数据，中断程序会唤醒等待队列中的进程A，同时它还会将发生事件的socketfd加入eventpoll的就绪列表rdlist，这样进程A可以直接从就绪列表上知道哪些socket发生了事件。

相较于select，虽然进程添加到eventpoll的等待队列仍然存在内核切换的开销，但这只会在epoll_create的时候发生一次，之后epoll_ctl操作的对象eventpoll本身就是文件描述符。select还没有进行监视和等待的拆分，导致每次要进行等待时都需要重新修改socket们的等待队列。最后一点就是，select没法知道是哪个socket上发生事件，需要进行遍历操作，而epoll通过rdlist很好的解决了这一点。

>epoll使用**红黑树**作为索引结构，搜索、插入、删除的时间复杂度都是**O(logN)**。
>
>epoll的两种工作模式：
>
>- LT水平触发：只要内核缓冲区有数据就会一直上报EPOLLIN事件。
>- ET边缘触发：只会在有新数据到来的时候上报一次

